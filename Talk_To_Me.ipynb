{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO13FrJeekMEctxBy+e2n5J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KamoLovesCode/dr/blob/main/Talk_To_Me.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bGBcwQH4YkS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72d9bcd3"
      },
      "source": [
        "!pip install openai-whisper transformers soundfile numpy torchaudio IPython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03b5174d"
      },
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab4a952f"
      },
      "source": [
        "from IPython.display import Javascript\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "\n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const record = async () => {\n",
        "  const options = {  mimeType: 'audio/webm; codecs=opus' }\n",
        "  const div = document.createElement('div')\n",
        "  const live = document.createElement('button')\n",
        "  live.appendChild(document.createTextNode('üé§ Record'))\n",
        "  div.appendChild(live)\n",
        "\n",
        "  const rec = document.createElement('button')\n",
        "  rec.appendChild(document.createTextNode('‚èπ Stop'))\n",
        "  div.appendChild(rec)\n",
        "\n",
        "  const msg = document.createElement('div')\n",
        "  msg.appendChild(document.createTextNode('Press üé§ to start recording'))\n",
        "  div.appendChild(msg)\n",
        "  output.appendChild(div)\n",
        "\n",
        "  const stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  const recorder = new MediaRecorder(stream, options)\n",
        "  let  data = []\n",
        "  recorder.ondataavailable = e => data.push(e.data)\n",
        "  recorder.start()\n",
        "  msg.textcontent = 'Recording...'\n",
        "\n",
        "  await new Promise(resolve => { live.onclick = resolve })\n",
        "  live.replaceWith(rec)\n",
        "  recorder.stop()\n",
        "\n",
        "  await sleep(1000)\n",
        "\n",
        "  const  blob = new Blob(data, options)\n",
        "  const  url = URL.createObjectURL(blob)\n",
        "  const a = document.createElement('a')\n",
        "  a.href = url\n",
        "  a.download = 'audio.webm'\n",
        "  a.click()\n",
        "  output.removeChild(div)\n",
        "  return await new Promise(resolve => {\n",
        "    const reader = new FileReader()\n",
        "    reader.readAsDataURL(blob)\n",
        "    reader.onloadend = () => resolve(reader.result.split(',', 1)[0])\n",
        "  })\n",
        "}\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09e6cfb1"
      },
      "source": [
        "Now run the cell below to record your voice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0828fe76"
      },
      "source": [
        "from IPython.display import Javascript, display\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "import threading\n",
        "\n",
        "def record_audio(filename='audio.webm'):\n",
        "    \"\"\"Records audio from the microphone in Colab and saves it to a file.\"\"\"\n",
        "\n",
        "    js_code = \"\"\"\n",
        "        const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "        const record = async () => {\n",
        "          const options = {  mimeType: 'audio/webm; codecs=opus' }\n",
        "          const div = document.createElement('div')\n",
        "          const live = document.createElement('button')\n",
        "          live.appendChild(document.createTextNode('üé§ Record'))\n",
        "          div.appendChild(live)\n",
        "\n",
        "          const rec = document.createElement('button')\n",
        "          rec.appendChild(document.createTextNode('‚èπ Stop'))\n",
        "          div.appendChild(rec)\n",
        "\n",
        "          const msg = document.createElement('div')\n",
        "          msg.appendChild(document.createTextNode('Press üé§ to start recording'))\n",
        "          div.appendChild(msg)\n",
        "          document.querySelector('#output-area').appendChild(div); // Append to the output area\n",
        "\n",
        "          const stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "          const recorder = new MediaRecorder(stream, options)\n",
        "          let  data = []\n",
        "          recorder.ondataavailable = e => data.push(e.data)\n",
        "          recorder.start()\n",
        "          msg.textContent = 'Recording...' // Use textContent\n",
        "\n",
        "          await new Promise(resolve => { live.onclick = resolve })\n",
        "          live.replaceWith(rec)\n",
        "          recorder.stop()\n",
        "\n",
        "          await sleep(1000)\n",
        "\n",
        "          const  blob = new Blob(data, options)\n",
        "          document.querySelector('#output-area').removeChild(div); // Remove from the output area\n",
        "\n",
        "          // Read the blob and send it to the Python callback\n",
        "          const reader = new FileReader();\n",
        "          reader.onloadend = () => {\n",
        "            google.colab.kernel.invokeFunction('notebook.record_audio_callback', [reader.result]);\n",
        "          };\n",
        "          reader.readAsDataURL(blob);\n",
        "        }\n",
        "        record();\n",
        "    \"\"\"\n",
        "\n",
        "    def record_audio_callback(data_url):\n",
        "        binary = b64decode(data_url.split(',')[1])\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(binary)\n",
        "        print(f\"Audio saved to {filename}\")\n",
        "\n",
        "    output.register_callback('notebook.record_audio_callback', record_audio_callback)\n",
        "    display(Javascript(js_code))\n",
        "\n",
        "# Use threading to prevent blocking the notebook\n",
        "threading.Thread(target=record_audio).start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a24cba67"
      },
      "source": [
        "!pip install openai-whisper transformers soundfile numpy torchaudio IPython\n",
        "\n",
        "import whisper\n",
        "\n",
        "# Load the Whisper model (ensure this is run in the same cell or before)\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "audio_file = \"audio.webm\"\n",
        "result = model.transcribe(audio_file)\n",
        "transcribed_text = result[\"text\"]\n",
        "print(f\"Transcribed Text: {transcribed_text}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0b41f96"
      },
      "source": [
        "Now we can use a language model to generate a response based on the transcribed text. You'll need an API key to use the Gemini API. If you don't already have one, create a key in Google AI Studio.\n",
        "\n",
        "In Colab, add the key to the secrets manager under the \"üîë\" in the left panel. Give it the name `GOOGLE_API_KEY`. Then pass the key to the SDK:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CcwqQ7BJ66Jr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dTsE_a90696_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3233160d"
      },
      "source": [
        "# Import the Python SDK\n",
        "import google.generativeai as genai\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55f205f8"
      },
      "source": [
        "Before you can make any API calls, you need to initialize the Generative Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "566d6bac"
      },
      "source": [
        "# Initialize the Gemini API\n",
        "# Using a suitable model for conversational tasks\n",
        "gemini_model = genai.GenerativeModel('gemini-1.5-flash-latest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e744c1f7"
      },
      "source": [
        "Now, let's generate a response using the transcribed text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c25611a"
      },
      "source": [
        "# Use the transcribed text as a prompt for the language model\n",
        "prompt = transcribed_text\n",
        "response = gemini_model.generate_content(prompt)\n",
        "\n",
        "# Print the generated response\n",
        "print(\"Generated Response:\")\n",
        "print(response.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e247171"
      },
      "source": [
        "# Task\n",
        "Create a simple web application using Python that records audio, transcribes it using the Whisper model, and then uses the Gemini model to generate a response based on the transcription. The application should display both the transcribed text and the generated response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9f5a31d"
      },
      "source": [
        "## Set up the web framework\n",
        "\n",
        "### Subtask:\n",
        "Install necessary libraries and set up a basic web framework (like Flask or Streamlit).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a77220b"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the Flask library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e728bfe"
      },
      "source": [
        "!pip install Flask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "933abb6d"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a basic Flask application in a new file and define a home route.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b94507a3"
      },
      "source": [
        "# This code would typically go into a file named app.py\n",
        "from flask import Flask\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def hello_world():\n",
        "    return 'Hello, World!'\n",
        "\n",
        "# To run this, you would typically use:\n",
        "# if __name__ == '__main__':\n",
        "#     app.run(debug=True)\n",
        "\n",
        "# Note: In a standard Colab environment, running Flask directly like this\n",
        "# might require additional setup for external access or using tools like ngrok.\n",
        "# For this subtask, we are just setting up the basic structure in a file."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56d471d4"
      },
      "source": [
        "## Create the web interface\n",
        "\n",
        "### Subtask:\n",
        "Design a simple interface for the web app, including elements for recording audio and displaying the transcribed text and generated response.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edcd3dc6"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the HTML template file with the necessary elements and basic styling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b62e69f5"
      },
      "source": [
        "%%writefile templates/index.html\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Voice Assistant</title>\n",
        "    <style>\n",
        "        body { font-family: Arial, sans-serif; margin: 20px; }\n",
        "        #controls { margin-bottom: 20px; }\n",
        "        #transcribed_text, #generated_response {\n",
        "            border: 1px solid #ccc;\n",
        "            padding: 10px;\n",
        "            margin-bottom: 10px;\n",
        "            min-height: 100px;\n",
        "            white-space: pre-wrap;\n",
        "        }\n",
        "        button { padding: 10px; cursor: pointer; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Voice Assistant</h1>\n",
        "\n",
        "    <div id=\"controls\">\n",
        "        <button id=\"recordButton\">üé§ Record</button>\n",
        "    </div>\n",
        "\n",
        "    <h2>Transcribed Text:</h2>\n",
        "    <div id=\"transcribed_text\"></div>\n",
        "\n",
        "    <h2>Generated Response:</h2>\n",
        "    <div id=\"generated_response\"></div>\n",
        "\n",
        "    <script>\n",
        "        const recordButton = document.getElementById('recordButton');\n",
        "        const transcribedTextDiv = document.getElementById('transcribed_text');\n",
        "        const generatedResponseDiv = document.getElementById('generated_response');\n",
        "        let mediaRecorder;\n",
        "        let audioChunks = [];\n",
        "\n",
        "        recordButton.onclick = async () => {\n",
        "            if (mediaRecorder && mediaRecorder.state === 'recording') {\n",
        "                mediaRecorder.stop();\n",
        "                recordButton.textContent = 'üé§ Record';\n",
        "            } else {\n",
        "                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
        "                mediaRecorder = new MediaRecorder(stream);\n",
        "                audioChunks = [];\n",
        "                mediaRecorder.ondataavailable = event => {\n",
        "                    audioChunks.push(event.data);\n",
        "                };\n",
        "                mediaRecorder.onstop = async () => {\n",
        "                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });\n",
        "                    // Here you would typically send the audioBlob to your Flask backend\n",
        "                    // For now, we'll just indicate recording stopped.\n",
        "                    transcribedTextDiv.textContent = 'Processing audio...';\n",
        "                    generatedResponseDiv.textContent = '';\n",
        "\n",
        "                    // Example of how you might send the blob to the backend (requires fetch API)\n",
        "                    // const formData = new FormData();\n",
        "                    // formData.append('audio', audioBlob, 'audio.webm');\n",
        "                    // fetch('/upload_audio', {\n",
        "                    //     method: 'POST',\n",
        "                    //     body: formData\n",
        "                    // })\n",
        "                    // .then(response => response.json())\n",
        "                    // .then(data => {\n",
        "                    //     transcribedTextDiv.textContent = data.transcription;\n",
        "                    //     generatedResponseDiv.textContent = data.response;\n",
        "                    // })\n",
        "                    // .catch(error => {\n",
        "                    //     transcribedTextDiv.textContent = 'Error processing audio.';\n",
        "                    //     generatedResponseDiv.textContent = '';\n",
        "                    //     console.error('Error:', error);\n",
        "                    // });\n",
        "                };\n",
        "                mediaRecorder.start();\n",
        "                recordButton.textContent = '‚èπ Stop';\n",
        "            }\n",
        "        };\n",
        "    </script>\n",
        "</body>\n",
        "</html>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95955ff4"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because the 'templates' directory does not exist. Create the directory first and then create the HTML file within it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b-Lj3Jc7ckg"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create the templates directory if it doesn't exist\n",
        "if not os.path.exists('templates'):\n",
        "    os.makedirs('templates')\n",
        "\n",
        "# Now write the HTML file into the templates directory\n",
        "%%writefile templates/index.html\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Voice Assistant</title>\n",
        "    <style>\n",
        "        body { font-family: Arial, sans-serif; margin: 20px; }\n",
        "        #controls { margin-bottom: 20px; }\n",
        "        #transcribed_text, #generated_response {\n",
        "            border: 1px solid #ccc;\n",
        "            padding: 10px;\n",
        "            margin-bottom: 10px;\n",
        "            min-height: 100px;\n",
        "            white-space: pre-wrap;\n",
        "        }\n",
        "        button { padding: 10px; cursor: pointer; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Voice Assistant</h1>\n",
        "\n",
        "    <div id=\"controls\">\n",
        "        <button id=\"recordButton\">üé§ Record</button>\n",
        "    </div>\n",
        "\n",
        "    <h2>Transcribed Text:</h2>\n",
        "    <div id=\"transcribed_text\"></div>\n",
        "\n",
        "    <h2>Generated Response:</h2>\n",
        "    <div id=\"generated_response\"></div>\n",
        "\n",
        "    <script>\n",
        "        const recordButton = document.getElementById('recordButton');\n",
        "        const transcribedTextDiv = document.getElementById('transcribed_text');\n",
        "        const generatedResponseDiv = document.getElementById('generated_response');\n",
        "        let mediaRecorder;\n",
        "        let audioChunks = [];\n",
        "\n",
        "        recordButton.onclick = async () => {\n",
        "            if (mediaRecorder && mediaRecorder.state === 'recording') {\n",
        "                mediaRecorder.stop();\n",
        "                recordButton.textContent = 'üé§ Record';\n",
        "            } else {\n",
        "                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
        "                mediaRecorder = new MediaRecorder(stream);\n",
        "                audioChunks = [];\n",
        "                mediaRecorder.ondataavailable = event => {\n",
        "                    audioChunks.push(event.data);\n",
        "                };\n",
        "                mediaRecorder.onstop = async () => {\n",
        "                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });\n",
        "                    // Here you would typically send the audioBlob to your Flask backend\n",
        "                    // For now, we'll just indicate recording stopped.\n",
        "                    transcribedTextDiv.textContent = 'Processing audio...';\n",
        "                    generatedResponseDiv.textContent = '';\n",
        "\n",
        "                    // Example of how you might send the blob to the backend (requires fetch API)\n",
        "                    // const formData = new FormData();\n",
        "                    // formData.append('audio', audioBlob, 'audio.webm');\n",
        "                    // fetch('/upload_audio', {\n",
        "                    //     method: 'POST',\n",
        "                    //     body: formData\n",
        "                    // })\n",
        "                    // .then(response => response.json())\n",
        "                    // .then(data => {\n",
        "                    //     transcribedTextDiv.textContent = data.transcription;\n",
        "                    //     generatedResponseDiv.textContent = data.response;\n",
        "                    // })\n",
        "                    // .catch(error => {\n",
        "                    //     transcribedTextDiv.textContent = 'Error processing audio.';\n",
        "                    //     generatedResponseDiv.textContent = '';\n",
        "                    //     console.error('Error:', error);\n",
        "                    // });\n",
        "                };\n",
        "                mediaRecorder.start();\n",
        "                recordButton.textContent = '‚èπ Stop';\n",
        "            }\n",
        "        };\n",
        "    </script>\n",
        "</body>\n",
        "</html>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfb83cb5"
      },
      "source": [
        "**Reasoning**:\n",
        "The error is a `SyntaxError: invalid decimal literal` caused by mixing Python code and the `%%writefile` magic command within the same cell without a blank line in between. Separate the Python code from the magic command with a blank line.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C4KffWn7exF"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create the templates directory if it doesn't exist\n",
        "if not os.path.exists('templates'):\n",
        "    os.makedirs('templates')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "462ca06d"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the directory is created, write the HTML template file into the 'templates' directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b225f03c"
      },
      "source": [
        "%%writefile templates/index.html\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Voice Assistant</title>\n",
        "    <style>\n",
        "        body { font-family: Arial, sans-serif; margin: 20px; }\n",
        "        #controls { margin-bottom: 20px; }\n",
        "        #transcribed_text, #generated_response {\n",
        "            border: 1px solid #ccc;\n",
        "            padding: 10px;\n",
        "            margin-bottom: 10px;\n",
        "            min-height: 100px;\n",
        "            white-space: pre-wrap;\n",
        "        }\n",
        "        button { padding: 10px; cursor: pointer; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Voice Assistant</h1>\n",
        "\n",
        "    <div id=\"controls\">\n",
        "        <button id=\"recordButton\">üé§ Record</button>\n",
        "    </div>\n",
        "\n",
        "    <h2>Transcribed Text:</h2>\n",
        "    <div id=\"transcribed_text\"></div>\n",
        "\n",
        "    <h2>Generated Response:</h2>\n",
        "    <div id=\"generated_response\"></div>\n",
        "\n",
        "    <script>\n",
        "        const recordButton = document.getElementById('recordButton');\n",
        "        const transcribedTextDiv = document.getElementById('transcribed_text');\n",
        "        const generatedResponseDiv = document.getElementById('generated_response');\n",
        "        let mediaRecorder;\n",
        "        let audioChunks = [];\n",
        "\n",
        "        recordButton.onclick = async () => {\n",
        "            if (mediaRecorder && mediaRecorder.state === 'recording') {\n",
        "                mediaRecorder.stop();\n",
        "                recordButton.textContent = 'üé§ Record';\n",
        "            } else {\n",
        "                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
        "                mediaRecorder = new MediaRecorder(stream);\n",
        "                audioChunks = [];\n",
        "                mediaRecorder.ondataavailable = event => {\n",
        "                    audioChunks.push(event.data);\n",
        "                };\n",
        "                mediaRecorder.onstop = async () => {\n",
        "                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });\n",
        "                    // Here you would typically send the audioBlob to your Flask backend\n",
        "                    // For now, we'll just indicate recording stopped.\n",
        "                    transcribedTextDiv.textContent = 'Processing audio...';\n",
        "                    generatedResponseDiv.textContent = '';\n",
        "\n",
        "                    // Example of how you might send the blob to the backend (requires fetch API)\n",
        "                    // const formData = new FormData();\n",
        "                    // formData.append('audio', audioBlob, 'audio.webm');\n",
        "                    // fetch('/upload_audio', {\n",
        "                    //     method: 'POST',\n",
        "                    //     body: formData\n",
        "                    // })\n",
        "                    // .then(response => response.json())\n",
        "                    // .then(data => {\n",
        "                    //     transcribedTextDiv.textContent = data.transcription;\n",
        "                    //     generatedResponseDiv.textContent = data.response;\n",
        "                    // })\n",
        "                    // .catch(error => {\n",
        "                    //     transcribedTextDiv.textContent = 'Error processing audio.';\n",
        "                    //     generatedResponseDiv.textContent = '';\n",
        "                    //     console.error('Error:', error);\n",
        "                    // });\n",
        "                };\n",
        "                mediaRecorder.start();\n",
        "                recordButton.textContent = '‚èπ Stop';\n",
        "            }\n",
        "        };\n",
        "    </script>\n",
        "</body>\n",
        "</html>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f5c66dc"
      },
      "source": [
        "**Reasoning**:\n",
        "The HTML template file has been created. The next step is to configure the Flask application to render this template when the root URL is accessed. This involves modifying the Flask application code. Since the Flask app was set up in a previous subtask, we will create a new file or modify the existing one to include the rendering logic. We'll create a new file named `app.py` and include the `render_template` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18bc29a7"
      },
      "source": [
        "%%writefile app.py\n",
        "from flask import Flask, render_template, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "# In future steps, we will add routes for handling audio upload and processing\n",
        "# @app.route('/upload_audio', methods=['POST'])\n",
        "# def upload_audio():\n",
        "#     if 'audio' not in request.files:\n",
        "#         return jsonify({\"error\": \"No audio file part\"}), 400\n",
        "#\n",
        "#     audio_file = request.files['audio']\n",
        "#     if audio_file.filename == '':\n",
        "#         return jsonify({\"error\": \"No selected file\"}), 400\n",
        "#\n",
        "#     # Save the audio file and process it with Whisper and Gemini\n",
        "#     # This part will be implemented in later subtasks\n",
        "#     # transcribed_text = transcribe_audio(audio_file)\n",
        "#     # generated_response = generate_response(transcribed_text)\n",
        "#\n",
        "#     # return jsonify({\"transcription\": transcribed_text, \"response\": generated_response})\n",
        "#     return jsonify({\"message\": \"Audio uploaded and processed (placeholder)\"}) # Placeholder\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # This is for running the Flask app directly.\n",
        "    # In a Colab environment, you might need a different approach for external access.\n",
        "    app.run(debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea65bae1"
      },
      "source": [
        "## Integrate audio recording\n",
        "\n",
        "### Subtask:\n",
        "Implement the functionality to record audio directly within the web app.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a51d8376"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `index.html` file to include JavaScript for audio recording and sending the data to the backend, and update `app.py` with a route to handle the audio upload.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65d67821"
      },
      "source": [
        "%%writefile templates/index.html\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Voice Assistant</title>\n",
        "    <style>\n",
        "        body { font-family: Arial, sans-serif; margin: 20px; }\n",
        "        #controls { margin-bottom: 20px; }\n",
        "        #transcribed_text, #generated_response {\n",
        "            border: 1px solid #ccc;\n",
        "            padding: 10px;\n",
        "            margin-bottom: 10px;\n",
        "            min-height: 100px;\n",
        "            white-space: pre-wrap;\n",
        "        }\n",
        "        button { padding: 10px; cursor: pointer; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Voice Assistant</h1>\n",
        "\n",
        "    <div id=\"controls\">\n",
        "        <button id=\"recordButton\">üé§ Record</button>\n",
        "    </div>\n",
        "\n",
        "    <h2>Transcribed Text:</h2>\n",
        "    <div id=\"transcribed_text\"></div>\n",
        "\n",
        "    <h2>Generated Response:</h2>\n",
        "    <div id=\"generated_response\"></div>\n",
        "\n",
        "    <script>\n",
        "        const recordButton = document.getElementById('recordButton');\n",
        "        const transcribedTextDiv = document.getElementById('transcribed_text');\n",
        "        const generatedResponseDiv = document.getElementById('generated_response');\n",
        "        let mediaRecorder;\n",
        "        let audioChunks = [];\n",
        "\n",
        "        recordButton.onclick = async () => {\n",
        "            if (mediaRecorder && mediaRecorder.state === 'recording') {\n",
        "                mediaRecorder.stop();\n",
        "                recordButton.textContent = 'üé§ Record';\n",
        "            } else {\n",
        "                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
        "                mediaRecorder = new MediaRecorder(stream);\n",
        "                audioChunks = [];\n",
        "                mediaRecorder.ondataavailable = event => {\n",
        "                    audioChunks.push(event.data);\n",
        "                };\n",
        "                mediaRecorder.onstop = async () => {\n",
        "                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });\n",
        "\n",
        "                    transcribedTextDiv.textContent = 'Processing audio...';\n",
        "                    generatedResponseDiv.textContent = '';\n",
        "\n",
        "                    const formData = new FormData();\n",
        "                    formData.append('audio', audioBlob, 'audio.webm');\n",
        "\n",
        "                    fetch('/upload_audio', {\n",
        "                        method: 'POST',\n",
        "                        body: formData\n",
        "                    })\n",
        "                    .then(response => response.json())\n",
        "                    .then(data => {\n",
        "                         // Update these lines in subsequent steps with actual transcription and response\n",
        "                        transcribedTextDiv.textContent = data.message; // Using message for now\n",
        "                        generatedResponseDiv.textContent = 'Waiting for processing...'; // Placeholder\n",
        "                    })\n",
        "                    .catch(error => {\n",
        "                        transcribedTextDiv.textContent = 'Error processing audio.';\n",
        "                        generatedResponseDiv.textContent = '';\n",
        "                        console.error('Error:', error);\n",
        "                    });\n",
        "                };\n",
        "                mediaRecorder.start();\n",
        "                recordButton.textContent = '‚èπ Stop';\n",
        "            }\n",
        "        };\n",
        "    </script>\n",
        "</body>\n",
        "</html>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59beea6c"
      },
      "source": [
        "**Reasoning**:\n",
        "Create or modify the `app.py` file to include the `/upload_audio` route to receive and save the audio file sent from the frontend.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff8fe2a5"
      },
      "source": [
        "%%writefile app.py\n",
        "from flask import Flask, render_template, request, jsonify\n",
        "import os\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/upload_audio', methods=['POST'])\n",
        "def upload_audio():\n",
        "    if 'audio' not in request.files:\n",
        "        return jsonify({\"error\": \"No audio file part\"}), 400\n",
        "\n",
        "    audio_file = request.files['audio']\n",
        "    if audio_file.filename == '':\n",
        "        return jsonify({\"error\": \"No selected file\"}), 400\n",
        "\n",
        "    # Define the path to save the audio file\n",
        "    audio_path = 'audio.webm'\n",
        "\n",
        "    try:\n",
        "        audio_file.save(audio_path)\n",
        "        print(f\"Audio file saved to {audio_path}\") # Log successful save\n",
        "        # In subsequent steps, we will process this file\n",
        "        return jsonify({\"message\": f\"Audio file received and saved as {audio_path}\"})\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving audio file: {e}\") # Log the error\n",
        "        return jsonify({\"error\": \"Failed to save audio file\"}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # This is for running the Flask app directly.\n",
        "    # In a Colab environment, you might need a different approach for external access.\n",
        "    # For development, you can run this and use ngrok to expose the port.\n",
        "    app.run(debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c99e7939"
      },
      "source": [
        "## Integrate whisper transcription\n",
        "\n",
        "### Subtask:\n",
        "Use the recorded audio as input for the Whisper model to get the transcribed text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d288701"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `upload_audio` route in `app.py` to load the Whisper model, transcribe the saved audio file, store the transcribed text, and include it in the JSON response.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1fd3a38"
      },
      "source": [
        "%%writefile app.py\n",
        "from flask import Flask, render_template, request, jsonify\n",
        "import os\n",
        "import whisper # Import whisper\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the Whisper model globally to avoid reloading on each request\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/upload_audio', methods=['POST'])\n",
        "def upload_audio():\n",
        "    if 'audio' not in request.files:\n",
        "        return jsonify({\"error\": \"No audio file part\"}), 400\n",
        "\n",
        "    audio_file = request.files['audio']\n",
        "    if audio_file.filename == '':\n",
        "        return jsonify({\"error\": \"No selected file\"}), 400\n",
        "\n",
        "    # Define the path to save the audio file\n",
        "    audio_path = 'audio.webm'\n",
        "\n",
        "    try:\n",
        "        audio_file.save(audio_path)\n",
        "        print(f\"Audio file saved to {audio_path}\") # Log successful save\n",
        "\n",
        "        # Transcribe the audio file using Whisper\n",
        "        result = model.transcribe(audio_path)\n",
        "        transcribed_text = result[\"text\"]\n",
        "        print(f\"Transcribed Text: {transcribed_text}\") # Log transcribed text\n",
        "\n",
        "        # Remove the audio file after transcription to save space\n",
        "        os.remove(audio_path)\n",
        "        print(f\"Audio file {audio_path} removed.\")\n",
        "\n",
        "        # Include the transcribed text in the JSON response\n",
        "        return jsonify({\"transcription\": transcribed_text, \"message\": f\"Audio file received and transcribed.\"})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing audio file: {e}\") # Log the error\n",
        "        # Ensure to remove the file even if transcription fails\n",
        "        if os.path.exists(audio_path):\n",
        "            os.remove(audio_path)\n",
        "            print(f\"Audio file {audio_path} removed after error.\")\n",
        "        return jsonify({\"error\": \"Failed to process audio file\"}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # This is for running the Flask app directly.\n",
        "    # In a Colab environment, you might need a different approach for external access.\n",
        "    # For development, you can run this and use ngrok to expose the port.\n",
        "    # app.run(debug=True)\n",
        "    # Use a different port if needed, e.g., port=5001\n",
        "    app.run(host='0.0.0.0', port=5000, debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8d70f53"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the `index.html` file to display the transcribed text received from the backend.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c572989"
      },
      "source": [
        "%%writefile templates/index.html\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Voice Assistant</title>\n",
        "    <style>\n",
        "        body { font-family: Arial, sans-serif; margin: 20px; }\n",
        "        #controls { margin-bottom: 20px; }\n",
        "        #transcribed_text, #generated_response {\n",
        "            border: 1px solid #ccc;\n",
        "            padding: 10px;\n",
        "            margin-bottom: 10px;\n",
        "            min-height: 100px;\n",
        "            white-space: pre-wrap;\n",
        "        }\n",
        "        button { padding: 10px; cursor: pointer; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Voice Assistant</h1>\n",
        "\n",
        "    <div id=\"controls\">\n",
        "        <button id=\"recordButton\">üé§ Record</button>\n",
        "    </div>\n",
        "\n",
        "    <h2>Transcribed Text:</h2>\n",
        "    <div id=\"transcribed_text\"></div>\n",
        "\n",
        "    <h2>Generated Response:</h2>\n",
        "    <div id=\"generated_response\"></div>\n",
        "\n",
        "    <script>\n",
        "        const recordButton = document.getElementById('recordButton');\n",
        "        const transcribedTextDiv = document.getElementById('transcribed_text');\n",
        "        const generatedResponseDiv = document.getElementById('generated_response');\n",
        "        let mediaRecorder;\n",
        "        let audioChunks = [];\n",
        "\n",
        "        recordButton.onclick = async () => {\n",
        "            if (mediaRecorder && mediaRecorder.state === 'recording') {\n",
        "                mediaRecorder.stop();\n",
        "                recordButton.textContent = 'üé§ Record';\n",
        "            } else {\n",
        "                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
        "                mediaRecorder = new MediaRecorder(stream);\n",
        "                audioChunks = [];\n",
        "                mediaRecorder.ondataavailable = event => {\n",
        "                    audioChunks.push(event.data);\n",
        "                };\n",
        "                mediaRecorder.onstop = async () => {\n",
        "                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });\n",
        "\n",
        "                    transcribedTextDiv.textContent = 'Processing audio...';\n",
        "                    generatedResponseDiv.textContent = ''; // Clear previous response\n",
        "\n",
        "                    const formData = new FormData();\n",
        "                    formData.append('audio', audioBlob, 'audio.webm');\n",
        "\n",
        "                    fetch('/upload_audio', {\n",
        "                        method: 'POST',\n",
        "                        body: formData\n",
        "                    })\n",
        "                    .then(response => response.json())\n",
        "                    .then(data => {\n",
        "                        if (data.transcription) {\n",
        "                            transcribedTextDiv.textContent = data.transcription;\n",
        "                        } else {\n",
        "                            transcribedTextDiv.textContent = data.error || 'Error processing audio.';\n",
        "                        }\n",
        "                        generatedResponseDiv.textContent = 'Waiting for response generation...'; // Placeholder for next step\n",
        "                    })\n",
        "                    .catch(error => {\n",
        "                        transcribedTextDiv.textContent = 'Error processing audio.';\n",
        "                        generatedResponseDiv.textContent = '';\n",
        "                        console.error('Error:', error);\n",
        "                    });\n",
        "                };\n",
        "                mediaRecorder.start();\n",
        "                recordButton.textContent = '‚èπ Stop';\n",
        "            }\n",
        "        };\n",
        "    </script>\n",
        "</body>\n",
        "</html>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5613beb"
      },
      "source": [
        "## Integrate gemini model\n",
        "\n",
        "### Subtask:\n",
        "Use the transcribed text as input for the Gemini model to generate a response.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d72bc04"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify `app.py` to include the Gemini model, use the transcribed text to generate a response, and include the response in the JSON output. Update `index.html` to display the generated response.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66201089"
      },
      "source": [
        "%%writefile app.py\n",
        "from flask import Flask, render_template, request, jsonify\n",
        "import os\n",
        "import whisper\n",
        "import google.generativeai as genai # Import genai\n",
        "from google.colab import userdata # Import userdata for API key\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the Whisper model globally\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "\n",
        "# Configure and load the Gemini model globally\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "gemini_model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/upload_audio', methods=['POST'])\n",
        "def upload_audio():\n",
        "    if 'audio' not in request.files:\n",
        "        return jsonify({\"error\": \"No audio file part\"}), 400\n",
        "\n",
        "    audio_file = request.files['audio']\n",
        "    if audio_file.filename == '':\n",
        "        return jsonify({\"error\": \"No selected file\"}), 400\n",
        "\n",
        "    # Define the path to save the audio file\n",
        "    audio_path = 'audio.webm'\n",
        "\n",
        "    try:\n",
        "        audio_file.save(audio_path)\n",
        "        print(f\"Audio file saved to {audio_path}\") # Log successful save\n",
        "\n",
        "        # Transcribe the audio file using Whisper\n",
        "        result = whisper_model.transcribe(audio_path)\n",
        "        transcribed_text = result[\"text\"]\n",
        "        print(f\"Transcribed Text: {transcribed_text}\") # Log transcribed text\n",
        "\n",
        "        # Generate a response using the Gemini model\n",
        "        # Use a simple prompt, you can make this more complex\n",
        "        prompt = transcribed_text\n",
        "        gemini_response = gemini_model.generate_content(prompt)\n",
        "        generated_response_text = gemini_response.text\n",
        "        print(f\"Generated Response: {generated_response_text}\") # Log generated response\n",
        "\n",
        "\n",
        "        # Remove the audio file after transcription to save space\n",
        "        os.remove(audio_path)\n",
        "        print(f\"Audio file {audio_path} removed.\")\n",
        "\n",
        "        # Include both the transcribed text and generated response in the JSON response\n",
        "        return jsonify({\"transcription\": transcribed_text, \"response\": generated_response_text, \"message\": \"Audio processed and response generated.\"})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing audio file: {e}\") # Log the error\n",
        "        # Ensure to remove the file even if transcription or generation fails\n",
        "        if os.path.exists(audio_path):\n",
        "            os.remove(audio_path)\n",
        "            print(f\"Audio file {audio_path} removed after error.\")\n",
        "        return jsonify({\"error\": \"Failed to process audio file\"}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # This is for running the Flask app directly.\n",
        "    # In a Colab environment, you might need a different approach for external access.\n",
        "    # For development, you can run this and use ngrok to expose the port.\n",
        "    # app.run(debug=True)\n",
        "    # Use a different port if needed, e.g., port=5001\n",
        "    app.run(host='0.0.0.0', port=5000, debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3601d5b9"
      },
      "source": [
        "**Reasoning**:\n",
        "Update `index.html` to extract and display the generated response from the JSON payload.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "516d76f5"
      },
      "source": [
        "%%writefile templates/index.html\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Voice Assistant</title>\n",
        "    <style>\n",
        "        body { font-family: Arial, sans-serif; margin: 20px; }\n",
        "        #controls { margin-bottom: 20px; }\n",
        "        #transcribed_text, #generated_response {\n",
        "            border: 1px solid #ccc;\n",
        "            padding: 10px;\n",
        "            margin-bottom: 10px;\n",
        "            min-height: 100px;\n",
        "            white-space: pre-wrap;\n",
        "        }\n",
        "        button { padding: 10px; cursor: pointer; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Voice Assistant</h1>\n",
        "\n",
        "    <div id=\"controls\">\n",
        "        <button id=\"recordButton\">üé§ Record</button>\n",
        "    </div>\n",
        "\n",
        "    <h2>Transcribed Text:</h2>\n",
        "    <div id=\"transcribed_text\"></div>\n",
        "\n",
        "    <h2>Generated Response:</h2>\n",
        "    <div id=\"generated_response\"></div>\n",
        "\n",
        "    <script>\n",
        "        const recordButton = document.getElementById('recordButton');\n",
        "        const transcribedTextDiv = document.getElementById('transcribed_text');\n",
        "        const generatedResponseDiv = document.getElementById('generated_response');\n",
        "        let mediaRecorder;\n",
        "        let audioChunks = [];\n",
        "\n",
        "        recordButton.onclick = async () => {\n",
        "            if (mediaRecorder && mediaRecorder.state === 'recording') {\n",
        "                mediaRecorder.stop();\n",
        "                recordButton.textContent = 'üé§ Record';\n",
        "            } else {\n",
        "                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
        "                mediaRecorder = new MediaRecorder(stream);\n",
        "                audioChunks = [];\n",
        "                mediaRecorder.ondataavailable = event => {\n",
        "                    audioChunks.push(event.data);\n",
        "                };\n",
        "                mediaRecorder.onstop = async () => {\n",
        "                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });\n",
        "\n",
        "                    transcribedTextDiv.textContent = 'Processing audio...';\n",
        "                    generatedResponseDiv.textContent = ''; // Clear previous response\n",
        "\n",
        "                    const formData = new FormData();\n",
        "                    formData.append('audio', audioBlob, 'audio.webm');\n",
        "\n",
        "                    fetch('/upload_audio', {\n",
        "                        method: 'POST',\n",
        "                        body: formData\n",
        "                    })\n",
        "                    .then(response => response.json())\n",
        "                    .then(data => {\n",
        "                        if (data.transcription) {\n",
        "                            transcribedTextDiv.textContent = data.transcription;\n",
        "                        } else {\n",
        "                            transcribedTextDiv.textContent = data.error || 'Error processing audio.';\n",
        "                        }\n",
        "\n",
        "                        // Update to display the generated response\n",
        "                        if (data.response) {\n",
        "                            generatedResponseDiv.textContent = data.response;\n",
        "                        } else {\n",
        "                             generatedResponseDiv.textContent = data.error || 'Error generating response.';\n",
        "                        }\n",
        "                    })\n",
        "                    .catch(error => {\n",
        "                        transcribedTextDiv.textContent = 'Error processing audio.';\n",
        "                        generatedResponseDiv.textContent = '';\n",
        "                        console.error('Error:', error);\n",
        "                    });\n",
        "                };\n",
        "                mediaRecorder.start();\n",
        "                recordButton.textContent = '‚èπ Stop';\n",
        "            }\n",
        "        };\n",
        "    </script>\n",
        "</body>\n",
        "</html>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79961f62"
      },
      "source": [
        "## Display results\n",
        "\n",
        "### Subtask:\n",
        "Display the transcribed text and the generated response on the web interface.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2831071b"
      },
      "source": [
        "## Run the web app\n",
        "\n",
        "### Subtask:\n",
        "Provide instructions on how to run the web app locally or deploy it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d1bdb29"
      },
      "source": [
        "**Reasoning**:\n",
        "Provide instructions on how to run the Flask application locally and how to access it, including the case of running it in a Colab-like environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6108333"
      },
      "source": [
        "print(\"\"\"\n",
        "To run the web application locally:\n",
        "\n",
        "1.  Ensure you have Python installed.\n",
        "2.  Save the `app.py` and `templates/index.html` files in the same directory structure (create a 'templates' folder and put 'index.html' inside it).\n",
        "3.  Open a terminal or command prompt in the directory where `app.py` is saved.\n",
        "4.  Run the Flask application using the command:\n",
        "    ```bash\n",
        "    python app.py\n",
        "    ```\n",
        "5.  You will see output in the terminal indicating that the Flask development server is running. It will typically show something like:\n",
        "    ```\n",
        "     * Serving Flask app 'app'\n",
        "     * Debug mode: on\n",
        "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
        "     * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
        "    ```\n",
        "6.  Open a web browser and go to the address shown in the output, which is usually `http://127.0.0.1:5000/`.\n",
        "\n",
        "Interacting with the web app:\n",
        "\n",
        "*   Once the page loads, you will see the \"Voice Assistant\" title and a \"üé§ Record\" button.\n",
        "*   Click the \"üé§ Record\" button to start recording your voice. You might need to grant microphone permissions to your browser.\n",
        "*   Click the \"‚èπ Stop\" button to stop recording.\n",
        "*   The application will then process the audio. The \"Transcribed Text\" area will first show \"Processing audio...\" and then display the text transcribed by the Whisper model.\n",
        "*   The \"Generated Response\" area will show \"Waiting for response generation...\" and then display the response generated by the Gemini model based on your transcribed text.\n",
        "\n",
        "Running in environments like Google Colab:\n",
        "\n",
        "If you are running this Flask application in an environment like Google Colab, the local address (like `http://127.0.0.1:5000/`) is not directly accessible from your web browser because it's running on a remote server. To access it from your local machine, you would typically need to use a service like ngrok to create a public URL that tunnels to your local Flask server.\n",
        "\n",
        "Steps using ngrok in Colab:\n",
        "\n",
        "1.  Install ngrok in your Colab notebook:\n",
        "    ```bash\n",
        "    !pip install pyngrok\n",
        "    ```\n",
        "2.  Import `ngrok` and run it, specifying the port your Flask app is running on (default is 5000):\n",
        "    ```python\n",
        "    from pyngrok import ngrok\n",
        "    # Terminate open tunnels if any\n",
        "    ngrok.kill()\n",
        "    # Replace 5000 with the port your Flask app is running on\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\" * ngrok tunnel established at: {public_url}\")\n",
        "    ```\n",
        "3.  Run your Flask app in a separate cell (or using threading/background process) so it doesn't block the notebook:\n",
        "    ```python\n",
        "    # Make sure app is defined as in app.py\n",
        "    # app.run(host='0.0.0.0', port=5000, debug=True)\n",
        "    # You might need to run this in a separate thread or use a tool like `run_with_ngrok`\n",
        "    # if you want it to run in the background in Colab.\n",
        "    # A simple way for demonstration in Colab is to put the app.run call\n",
        "    # in a cell below the ngrok cell and execute them sequentially.\n",
        "    # However, this will block the cell. For non-blocking, threading is better.\n",
        "    # Example using threading (requires 'import threading' at the top):\n",
        "    # threading.Thread(target=app.run, kwargs={'host':'0.0.0.0','port':5000,'debug':True}).start()\n",
        "\n",
        "    # A simpler approach for demonstration is to use a utility function if available\n",
        "    # or just run the app.run() call in a cell and be aware it blocks.\n",
        "    # For this example, assume app.py contains the if __name__ == '__main__': block\n",
        "    # and you would run that cell.\n",
        "    ```\n",
        "4.  Open the `public_url` provided by ngrok in your web browser to access the application.\n",
        "\n",
        "Remember to replace `5000` with the actual port number if you configured your Flask app to run on a different port.\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b00589e"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The Flask web framework was successfully set up and a basic application structure was created.\n",
        "*   A simple HTML interface was designed with elements for recording audio and displaying transcribed text and generated responses.\n",
        "*   JavaScript was integrated into the frontend to handle audio recording using the `MediaRecorder` API and send the recorded audio to the backend via a POST request.\n",
        "*   The backend Flask application was updated with an `/upload_audio` route to receive and save the audio file.\n",
        "*   The Whisper model was successfully integrated into the backend to transcribe the saved audio file.\n",
        "*   The Gemini model was integrated into the backend to generate a response based on the transcribed text.\n",
        "*   The backend now returns both the transcribed text and the generated response to the frontend.\n",
        "*   The frontend JavaScript was updated to display both the transcribed text and the generated response received from the backend.\n",
        "*   Instructions were provided on how to run the web application locally and how to access it from environments like Google Colab using ngrok.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Enhance the user interface with visual feedback during recording and processing states.\n",
        "*   Implement more robust error handling and potentially add features like language selection for transcription or prompt engineering for the Gemini model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98ddaf4e"
      },
      "source": [
        "!python app.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a5818ea"
      },
      "source": [
        "!pip install openai-whisper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2ce0914"
      },
      "source": [
        "!python app.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e0c7781"
      },
      "source": [
        "%%writefile app.py\n",
        "from flask import Flask, render_template, request, jsonify\n",
        "import os\n",
        "import whisper\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the Whisper model globally\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "\n",
        "# Configure and load the Gemini model globally\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "gemini_model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/upload_audio', methods=['POST'])\n",
        "def upload_audio():\n",
        "    if 'audio' not in request.files:\n",
        "        return jsonify({\"error\": \"No audio file part\"}), 400\n",
        "\n",
        "    audio_file = request.files['audio']\n",
        "    if audio_file.filename == '':\n",
        "        return jsonify({\"error\": \"No selected file\"}), 400\n",
        "\n",
        "    # Define the path to save the audio file\n",
        "    audio_path = 'audio.webm'\n",
        "\n",
        "    try:\n",
        "        audio_file.save(audio_path)\n",
        "        print(f\"Audio file saved to {audio_path}\")\n",
        "\n",
        "        # Transcribe the audio file using Whisper\n",
        "        result = whisper_model.transcribe(audio_path)\n",
        "        transcribed_text = result[\"text\"]\n",
        "        print(f\"Transcribed Text: {transcribed_text}\")\n",
        "\n",
        "        # Generate a response using the Gemini model\n",
        "        prompt = transcribed_text\n",
        "        gemini_response = gemini_model.generate_content(prompt)\n",
        "        generated_response_text = gemini_response.text\n",
        "        print(f\"Generated Response: {generated_response_text}\")\n",
        "\n",
        "        # Remove the audio file after transcription to save space\n",
        "        os.remove(audio_path)\n",
        "        print(f\"Audio file {audio_path} removed.\")\n",
        "\n",
        "        # Include both the transcribed text and generated response in the JSON response\n",
        "        return jsonify({\"transcription\": transcribed_text, \"response\": generated_response_text, \"message\": \"Audio processed and response generated.\"})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing audio file: {e}\")\n",
        "        # Ensure to remove the file even if transcription or generation fails\n",
        "        if os.path.exists(audio_path):\n",
        "            os.remove(audio_path)\n",
        "            print(f\"Audio file {audio_path} removed after error.\")\n",
        "        return jsonify({\"error\": \"Failed to process audio file\"}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000, debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e501665e"
      },
      "source": [
        "!python app.py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}